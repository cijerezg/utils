from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils as nn_util
import torch
import numpy as np
import pdb
import sys
sys.path.insert(1, '/home/carlos/Documents/utils/')
from NNs import autograd_hacks


class nn_model:
    """
    It handles neural networks. It has the train method, test
    method.

    Parameters
    ----------
    model: pytorch model 
       It can be read from a .pt file. Does not need to be on
       GPU, or ser to double.
    """
    def __init__(self, model):
        self.model = model
        print(sum(p.numel() for p in model.parameters()))
        self.device = torch.device('cuda')
        self.model = self.model.to(self.device)
        self.model = self.model.double()

    def train(self, data, epochs, lr, evaluate, batch=1024, save=False, path=None):
        """
        It trains the neural network.
        
        Parameters
        ----------
        data: list
           It contains all inputs and outputs. The first 
           dimension is the number of samples.
        epochs: integer
           Number of epochs to train.
        evaluate: function
           This function evalutes the model, and then it
           calculates the loss. The inputs are the data 
           in a list, and should return the loss.
        save: boolean
           Whether to save the model or not. If yes, a
           path should be provided
        path: string
           Only needed is model is saved.

        Returns
        -------
        loss: list
           The loss throughout training
        """
        dset = self.data_loader(data)
        loader = DataLoader(dset, batch_size=batch,  shuffle=False, num_workers=4)
        optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=0.0)
        train_loss = []
        self.model.train()
        for i in range(epochs):
            partial_loss = 0
            for batch, data in enumerate(loader):
                data = [data[j].to(self.device) for j in range(len(data))]
                optimizer.zero_grad()
                loss = evaluate(data, self.model)
                loss.backward(retain_graph=True)
                # autograd_hacks.compute_grad1(self.model)
                # for name, params in self.model.named_parameters():
                #     if "bias" in name:
                #         continue
                #     per_sample_grad = params.grad1
                #     gradients = params.grad.detach().clone()
                #     pdb.set_trace()
                #     print('hold on')
                partial_loss += loss
#                nn_util.clip_grad_norm_(self.model.parameters(), 5*np.log(i+2))
                optimizer.step()
            print('Epoch is {} and loss is {}'.format(i, round(partial_loss.item(),5)))
            train_loss.append(partial_loss.item())
        if save == True:
            torch.save(self.model, path)
        return train_loss
            
    def test(self, data, eval_test):
        """
        It uses model the evalute the data.

        Parameters
        ----------
        data_loader: array
           The first dimension is the number of samples
        eval_test: function
           This function evalutes the model, and then it
           calculates the loss. The inputs are the data in a list,
           and should return the loss, and the neural net output.
        
        Returns
        -------
        loss: array
          The loss for the testing samples
        output: array
          The neural network output
        """
        dset = self.data_loader(data)
        b_s = data[0].shape[0]
        loader = DataLoader(dset, batch_size=b_s, shuffle=False, num_workers=4)
        self.model.eval()
        with torch.no_grad():
            for data in loader:
                data = [data[j].to(self.device) for j in range(len(data))]
                loss, output = eval_test(data, self.model)
                loss = loss.cpu().detach().numpy()
                output = output.cpu().detach().numpy()
        return loss, output
                
    def data_loader(self, data):
        """
        Creates dataloader object from data organized in a list.
        """
        class DriveData(Dataset):
            def __init__(self, data, transform=None):
                self.xs = [torch.from_numpy(data[i]) for i in range(len(data))]

            def __getitem__(self, index):
                return [self.xs[i][index] for i in range(len(self.xs))]

            def __len__(self):
                return len(self.xs[0])
        dset = DriveData(data)
        return dset
